{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitcf688b4200924e1a9de8aa02cb2c3f5c",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #Tensorflow handles the Training and Testing\n",
    "from tensorflow import keras #Keras handles the importing of Data\n",
    "import numpy as np #NumPy does funny math good\n",
    "import gym #imports OpenAI Gym which has a bunch of environments(games) to play with\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, median \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment Notes:\n",
    "#   -new_state is an array of 4 values \n",
    "#       Num\t    Observation\t            Min\t        Max\n",
    "#       0\t    Cart Position\t        -2.4\t    2.4\n",
    "#       1\t    Cart Velocity\t        -Inf\t    Inf\n",
    "#       2\t    Pole Angle\t            ~ -41.8°\t~ 41.8°\n",
    "#       3\t    Pole Velocity At Tip\t-Inf\t    Inf\n",
    "# \n",
    "#   - actions are left or right mapped as 0=left 1=right\n",
    "# \n",
    "#   - Episodes are terminated if:\n",
    "#       - Pole Angle is more than ±12°\n",
    "#       - Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "#       - Episode length is greater than 200 (500 for v1).\n",
    "# \n",
    "#   - Problem is considered solved if:\n",
    "#       - Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Action Test\n",
    "# for e in range(10):\n",
    "#     state = env.reset()\n",
    "#     for s in range(100):\n",
    "\n",
    "#         env.render()\n",
    "#         action = env.action_space.sample()\n",
    "#         new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "#         # if done:\n",
    "#         #     break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Data Generation Progress: 100%|█████████▉| 274654/275000 [01:11<00:00, 3634.32it/s]\nTraining data has 10252 episodes (Loss = 96.272 %)\nAverage accepted score: 61.47639484978541\nMedian score for accepted scores: 58.0\n"
    }
   ],
   "source": [
    "#Hpyer Parameters\n",
    "score_threshhold = 50\n",
    "n_training_games = 275000\n",
    "n_training_steps = 500\n",
    "\n",
    "#Creating Training Data\n",
    "training_data = [] #Each Piece of data will be in format (observations, moves)\n",
    "\n",
    "loss = 0\n",
    "progress = tqdm(total=n_training_games, position=0, leave=False)\n",
    "progress.set_description(\"Data Generation Progress\")\n",
    "scores = [] #List of good data\n",
    "for _ in range(n_training_games): #Generate 20000 training data\n",
    "    score = 0 #Score counter over episode\n",
    "\n",
    "    memory = [] #List of all moves\n",
    "    prev_obs = [] #Last observation made\n",
    "\n",
    "    progress.update(1)\n",
    "\n",
    "    env.reset()\n",
    "    for _ in range(n_training_steps): #Runs 250 steps per episode\n",
    "        action = env.action_space.sample() #Chooses random action\n",
    "        new_state, reward, done, _ = env.step(action) #Runs random action\n",
    "\n",
    "        if(len(prev_obs)>0): #Except for the first time, add all sets of previous observations and current actions to memory\n",
    "            memory.append([prev_obs, action])\n",
    "        prev_obs = new_state #Update previous observation\n",
    "\n",
    "        score+=reward #Update Score\n",
    "        if done: break #Episode data is <= 250 observations \n",
    "\n",
    "    if score >= score_threshhold:  \n",
    "        scores.append(score)\n",
    "        \n",
    "        #Converting actions(left,right) to one-hot(0,1) representation\n",
    "        #NOTE: data[0] is the observation of size 4, data[1] is resulting left-right action\n",
    "        for data in memory:\n",
    "            if(data[1] == 0):\n",
    "                output=[1,0]\n",
    "            elif(data[1] == 1):\n",
    "                output=[0,1]\n",
    "\n",
    "        training_data.append([data[0], output]) #Compiles data for training set\n",
    "    \n",
    "    else: loss+=1\n",
    "\n",
    "print()\n",
    "print(\"Training data has\", len(training_data), \"episodes (Loss =\", (loss/n_training_games)*100, \"%)\")\n",
    "print('Average accepted score:', mean(scores))\n",
    "print('Median score for accepted scores:', median(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 4, 128)            256       \n_________________________________________________________________\ndropout (Dropout)            (None, 4, 128)            0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4, 256)            33024     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 4, 256)            0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 4, 512)            131584    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 4, 512)            0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 4, 256)            131328    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 4, 256)            0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 4, 128)            32896     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 4, 128)            0         \n_________________________________________________________________\noutput (Dense)               (None, 4, 2)              258       \n=================================================================\nTotal params: 329,346\nTrainable params: 329,346\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "#Making the Model\n",
    "model = tf.keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(4,1), name = \"input_layer\"),\n",
    "\n",
    "    keras.layers.Dense(128, activation='relu', name=\"dense_1\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(256, activation='relu', name=\"dense_2\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(512, activation='relu', name=\"dense_3\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(256, activation='relu', name=\"dense_4\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(128, activation='relu', name=\"dense_5\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(2, activation='softmax', name=\"output\"),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss =\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 41008 into shape (4,1)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3846c8993ff7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Splitting training_data in x and y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#converts states from [cart_pos, cart_vel, pole_pos, pole,vel] to [[cart_pos], [cart_vel], [pole_pos], [pole,vel]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtrain_moves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 41008 into shape (4,1)"
     ]
    }
   ],
   "source": [
    "#Training Model\n",
    "\n",
    "#Splitting training_data in x and y\n",
    "#converts states from [cart_pos, cart_vel, pole_pos, pole,vel] to [[cart_pos], [cart_vel], [pole_pos], [pole,vel]]\n",
    "train_states = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "train_moves = np.array([i[1] for i in training_data])\n",
    "\n",
    "model.fit(train_states, train_moves, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Learning\n",
    "EPISODES = 150000\n",
    "MAX_STEPS = 1000\n",
    "EPSILON = 0.9\n",
    "from tqdm import tqdm\n",
    "progress = tqdm(total=EPISODES, position=0, leave=False)\n",
    "progress.set_description(\"Progress\")\n",
    "\n",
    "#Q-Table\n",
    "ALPHA = 0.81 #Represents the Learning Rate. This determines how much the agent will explore. High LR means more exploration\n",
    "GAMMA = 0.96 #Represents the Discount Factor. This determines how much the agent values the future reward. High DF means future rewards are more heavily considered\n",
    "\n",
    "rewards = [] #Log of Rewards per Episode\n",
    "\n",
    "#Training\n",
    "for e in range(EPISODES):\n",
    "    progress.update(1)\n",
    "    \n",
    "    state = env.reset() #Resets Environment\n",
    "\n",
    "    for s in range(MAX_STEPS):\n",
    "        \n",
    "        #env.render() #Renders Environment. CAUTION: Rendering takes more time to train\n",
    "        \n",
    "        #Picks Action. Chooses a random action EPSILON% of the time. Otherwise chooses the max reward option\n",
    "        if(np.random.uniform(0,1) < EPSILON):\n",
    "            action = env.action_space.sample() #Takes a random action, samples from env.action_space which is a list of possible actions {0: Left, 1: Down, 2: Right, 3: Up}\n",
    "        else: \n",
    "            action = np.argmax(Q[state, : ])\n",
    "        new_state, reward, done, _ = env.step(action) #Takes the action\n",
    "\n",
    "        #Updating Q-Table\n",
    "        Q[state, action] = getQ(state, action)\n",
    "\n",
    "        #changes states\n",
    "        state = new_state\n",
    "\n",
    "        #Handles if game finished\n",
    "        if done:\n",
    "            rewards.append(reward) #Adds \n",
    "            EPSILON -= 1/EPISODES #Steps down the random action rate to prioritize rewards over exploration\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning\n",
    "def get_average(values):\n",
    "  return sum(values)/len(values)\n",
    "\n",
    "avg_rewards = []\n",
    "for i in range(0, len(rewards), 100):\n",
    "  avg_rewards.append(get_average(rewards[i:i+100])) \n",
    "\n",
    "plt.plot(avg_rewards)\n",
    "plt.ylabel('average reward')\n",
    "plt.xlabel('episodes (100\\'s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset() #Resets Environment\n",
    "\n",
    "for s in range(MAX_STEPS):\n",
    "    \n",
    "    env.render() #Renders Environment. CAUTION: Rendering takes more time to train\n",
    "    \n",
    "    #Picks Action based on max reward\n",
    "    action = np.argmax(Q[state, : ])\n",
    "    new_state, reward, done, _ = env.step(action) #Takes the action\n",
    "\n",
    "    #changes states\n",
    "    state = new_state\n",
    "\n",
    "    #Handles if game finished\n",
    "    if done:\n",
    "        break\n",
    "print(\"It took\", s, \"steps to finish\")\n",
    "env.render()"
   ]
  }
 ]
}