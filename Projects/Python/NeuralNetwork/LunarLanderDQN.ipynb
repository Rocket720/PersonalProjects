{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "95b8d84bee1d58f8d89b483cda57382351d62d124ec774fa40d1d8166926bd17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #Tensorflow handles the Training and Testing\n",
    "from tensorflow import keras #Keras handles the importing of Data\n",
    "import numpy as np #NumPy does funny math good\n",
    "import gym #imports OpenAI Gym which has a bunch of environments(games) to play with\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, median \n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of States: 8\nNumber of Actions per State: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2') #Creates Environment \"LunarLander-v2\" from OpenAI Gym\n",
    "\n",
    "#Environment Notes:\n",
    "#   - new_state is an array of 8 observations \n",
    "#       Num\t    Observation\n",
    "#       0\t    Lander X Coord\n",
    "#       1\t    Lander Y Coord\n",
    "#       2\t    Lander X Velocity\n",
    "#       3\t    Lander Y Velocity\n",
    "#       4\t    Lander Angle\n",
    "#       5\t    Lander Angular Velocity\n",
    "#       6\t    Left Lander Leg Grounded\n",
    "#       7\t    Right Lander Leg Grounded\n",
    "# \n",
    "#   - action_space is an array of 4 actions \n",
    "#       Num\t    Action\n",
    "#       0\t    Do Nothing/Coast\n",
    "#       1\t    Fire Left Engine\n",
    "#       2\t    Fire Bottom Engine\n",
    "#       3\t    Fire Right Engine\n",
    "# \n",
    "#   - Episodes are terminated if:\n",
    "#       - Pole Angle is more than ±12°\n",
    "#       - Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "#       - Episode length is greater than MAX_STEPS.\n",
    "# \n",
    "#   - Replays are considered valid if:\n",
    "#       - Mean reward is greater than or equal to 200 of the set\\\n",
    "print(\"Number of States:\", env.observation_space.shape[0])\n",
    "print(\"Number of Actions per State:\", env.action_space.n) "
   ]
  },
  {
   "source": [
    "# #Runs 5 games with bottom thruster firing \n",
    "# for _ in range(10000):\n",
    "#     env.reset()\n",
    "#     for s in range(300):\n",
    "        \n",
    "#         env.render(True) #Renders Environment. CAUTION: Rendering takes more time to train\n",
    "        \n",
    "#         #Picks Action based on max reward\n",
    "#         action = 0\n",
    "#         if s%3==0: #Fires every 3rd frame\n",
    "#             action = 2\n",
    "        \n",
    "#         new_state, reward, done, _ = env.step(action) #Takes the action\n",
    "\n",
    "#         #changes states\n",
    "#         state = new_state\n",
    "\n",
    "#         # #Handles if game finished\n",
    "#         # if done:\n",
    "#         #     break"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the DQN\n",
    "class DQN():\n",
    "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
    "        #Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.num_action_space = env.action_space.n\n",
    "        self.num_observation_space = env.observation_space.shape[0]\n",
    "\n",
    "        self.training_data = []\n",
    "        self.batch_size = 1000\n",
    "\n",
    "        self.counter = 0\n",
    "\n",
    "        #Creating DQN with Architecture 512-256-4\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(512, input_dim=env.observation_space.shape[0], activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(env.action_space.n, activation=\"linear\"))\n",
    "\n",
    "        #Compiling Model using MSE Loss and Adam Optimizer\n",
    "        model.compile(loss=keras.losses.mean_squared_error, optimizer=keras.optimizers.Adam(lr=lr))\n",
    "\n",
    "        self.model = model\n",
    "        #print(model.summary())\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.num_action_space)\n",
    "\n",
    "        predicted_actions = self.model.predict(state)\n",
    "        return np.argmax(predicted_actions[0])\n",
    "\n",
    "    def train(self, episodes = 500, reward_threshold = -200):\n",
    "        progress = tqdm(total=episodes, position=0, leave=False)\n",
    "        \n",
    "        for e in range(episodes):\n",
    "            progress.update(1)\n",
    "\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            memory = []\n",
    "            MAX_STEPS = 1000\n",
    "            state = np.reshape(state, [1, self.num_observation_space])\n",
    "\n",
    "            for s in range(MAX_STEPS):\n",
    "                action = self.get_action(state)\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                new_state = np.reshape(new_state, [1, self.num_observation_space])\n",
    "\n",
    "                memory.append((state, action, reward, new_state, done))\n",
    "                \n",
    "                episode_reward += reward\n",
    "\n",
    "                state = new_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if episode_reward < reward_threshold:\n",
    "                continue\n",
    "            \n",
    "            self.training_data += memory\n",
    "\n",
    "            if len(self.training_data) < self.batch_size:\n",
    "                sample = self.training_data\n",
    "            else:\n",
    "                sample = random.sample(self.training_data, self.batch_size)\n",
    "        \n",
    "            states = np.squeeze(np.squeeze(np.array([i[0] for i in sample])))\n",
    "            actions = np.array([i[1] for i in sample])\n",
    "            rewards = np.array([i[2] for i in sample])\n",
    "            new_states = np.squeeze(np.array([i[3] for i in sample]))\n",
    "            done_list = np.array([i[4] for i in sample])\n",
    "\n",
    "            target_vec = self.model.predict_on_batch(states)\n",
    "            targets = rewards + self.gamma * (np.amax(target_vec, axis=1)) * (1 - done_list)\n",
    "            indexes = np.array([i for i in range(len(states))])\n",
    "            target_vec[[indexes], [actions]] = targets\n",
    "            self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= (episodes-e)/episodes #self.epsilon_decay\n",
    "\n",
    "            self.counter+=1\n",
    "    \n",
    "        print(\" Loss Rate:\", str(100 - self.counter/episodes * 100) +\"%\")\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "lr = .005\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99\n",
    "\n",
    "model = DQN(env, lr, gamma, epsilon, epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 4/500 [00:01<02:56,  2.81it/s]WARNING:tensorflow:5 out of the last 3286 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DFE7785D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " 10%|█         | 51/500 [03:51<49:18,  6.59s/it]"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fb6113237a44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LLtrainedmodel.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-bf5d1fc50c6a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, episodes, reward_threshold)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-bf5d1fc50c6a>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_action_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mpredicted_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[0;32m   1597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1599\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1835\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m     \"\"\"\n\u001b[1;32m-> 1837\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4289\u001b[0m               type(self._map_func.output_structure)))\n\u001b[0;32m   4290\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4291\u001b[1;33m     variant_tensor = gen_dataset_ops.flat_map_dataset(\n\u001b[0m\u001b[0;32m   4292\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4293\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mflat_map_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2000\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2002\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2003\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FlatMapDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(episodes = 500, reward_threshold = -300)\n",
    "model.save(\"LLtrainedmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "                                                 Starting Testing of the trained model...\n",
      "0 \t: Episode || Reward:  -753.6814635891927\n",
      "1 \t: Episode || Reward:  -775.1586227258692\n",
      "2 \t: Episode || Reward:  -758.6793014317976\n",
      "3 \t: Episode || Reward:  -734.0107349397996\n",
      "4 \t: Episode || Reward:  -734.4132955368593\n",
      "5 \t: Episode || Reward:  -791.7691374129538\n",
      "6 \t: Episode || Reward:  -824.4951593408313\n",
      "7 \t: Episode || Reward:  -731.8698113506512\n",
      "8 \t: Episode || Reward:  -706.5983985492016\n",
      "9 \t: Episode || Reward:  -821.547729018784\n",
      "10 \t: Episode || Reward:  -858.302099874204\n",
      "11 \t: Episode || Reward:  -759.4636026793324\n",
      "12 \t: Episode || Reward:  -722.8004990615435\n",
      "13 \t: Episode || Reward:  -692.3688155077658\n",
      "14 \t: Episode || Reward:  -744.6977890659447\n",
      "15 \t: Episode || Reward:  -682.5004593775169\n",
      "16 \t: Episode || Reward:  -694.9436650279075\n",
      "17 \t: Episode || Reward:  -779.4650245738165\n",
      "18 \t: Episode || Reward:  -756.2659838291547\n",
      "19 \t: Episode || Reward:  -743.6246502601274\n",
      "20 \t: Episode || Reward:  -744.6161193744199\n",
      "21 \t: Episode || Reward:  -740.4557758380918\n",
      "22 \t: Episode || Reward:  -547.1556724528851\n",
      "23 \t: Episode || Reward:  -513.7855896543958\n",
      "24 \t: Episode || Reward:  -766.8912618369264\n"
     ]
    }
   ],
   "source": [
    "trained_model = load_model(\"LLtrainedmodel.h5\")\n",
    "rewards_list = []\n",
    "num_test_episode = 25\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "print(\"Starting Testing of the trained model...\")\n",
    "\n",
    "step_count = 1000\n",
    "\n",
    "for e in range(num_test_episode):\n",
    "    current_state = env.reset()\n",
    "    num_observation_space = env.observation_space.shape[0]\n",
    "    current_state = np.reshape(current_state, [1, num_observation_space])\n",
    "    reward_for_episode = 0\n",
    "    for step in range(step_count):\n",
    "        env.render()\n",
    "        selected_action = np.argmax(trained_model.predict(current_state)[0])\n",
    "        new_state, reward, done, info = env.step(selected_action)\n",
    "        new_state = np.reshape(new_state, [1, num_observation_space])\n",
    "        current_state = new_state\n",
    "        reward_for_episode += reward\n",
    "        if done:\n",
    "            break\n",
    "    rewards_list.append(reward_for_episode)\n",
    "    print(e, \"\\t: Episode || Reward: \", reward_for_episode)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "source": [
    "Saved Model Logs\n",
    "\n",
    "1. Threshold Set to +300. No episodes accepted into training set. Model was making randomized actions\n",
    "2. Threshold set to -200. 500 Episodes. Best model so far. It is able to control its vertical velocity well, but is still shaky on        roll and targetting the pad (High Score: 270.715)\n",
    "3. Threshold set to -200. 1000 Epsiodes. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}