{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "95b8d84bee1d58f8d89b483cda57382351d62d124ec774fa40d1d8166926bd17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #Tensorflow handles the Training and Testing\n",
    "from tensorflow import keras #Keras handles the importing of Data\n",
    "import numpy as np #NumPy does funny math good\n",
    "import gym #imports OpenAI Gym which has a bunch of environments(games) to play with\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, median \n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "from keras.activations import relu, linear\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "import random\n",
    "from collections import deque "
   ]
  },
  {
   "source": [
    "# Environment Notes:\n",
    "  - `new_state` is an array of 8 observations\n",
    "    - Num   Observation\n",
    "    - 0     Lander X Coord\n",
    "    - 1     Lander Y Coord\n",
    "    - 2     Lander X Velocity\n",
    "    - 3     Lander Y Velocity\n",
    "    - 4     Lander Angle\n",
    "    - 5     Lander Angular Velocity\n",
    "    - 6     Left Lander Leg Grounded\n",
    "    - 7     Right Lander Leg Grounded\n",
    "\n",
    "  - `action_space` is an array of 4 actions \n",
    "    - Num   Observation\n",
    "    - 0\t    Do Nothing/Coast\n",
    "    - 1\t    Fire Left Engine\n",
    "    - 2\t    Fire Bottom Engine\n",
    "    - 3\t    Fire Right Engine\n",
    "\n",
    "  - Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Episodes are terminated if episode length is greater than `MAX_STEPS` or lander exceedes boundaries.\n",
    "      \n",
    "```\n",
    "print(\"Number of States:\", env.observation_space.shape[0])`  ->  `Number of States: 8\n",
    "print(\"Number of Actions per State:\", env.action_space.n)`   ->  `Number of Actions per State: 4\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2') #Creates Environment \"LunarLander-v2\" from OpenAI Gym"
   ]
  },
  {
   "source": [
    "# #Runs 5 games with bottom thruster firing \n",
    "# for _ in range(10000):\n",
    "#     env.reset()\n",
    "#     for s in range(300):\n",
    "        \n",
    "#         env.render(True) #Renders Environment. CAUTION: Rendering takes more time to train\n",
    "        \n",
    "#         #Picks Action based on max reward\n",
    "#         action = 0\n",
    "#         if s%3==0: #Fires every 3rd frame\n",
    "#             action = 2\n",
    "        \n",
    "#         new_state, reward, done, _ = env.step(action) #Takes the action\n",
    "\n",
    "#         #changes states\n",
    "#         state = new_state\n",
    "\n",
    "#         # #Handles if game finished\n",
    "#         # if done:\n",
    "#         #     break"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the DQN\n",
    "class DQN():\n",
    "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # Environment variables\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.num_action_space = env.action_space.n\n",
    "        self.num_observation_space = env.observation_space.shape[0]\n",
    "\n",
    "        # Training Variables \n",
    "        self.training_data = deque(maxlen=500000)\n",
    "        self.rewards_list = []\n",
    "        self.batch_size = 64\n",
    "        self.high_score = -8000\n",
    "\n",
    "        # Creating DQN with Architecture 512-256-4\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(512, input_dim=self.num_observation_space, activation=relu))\n",
    "        model.add(keras.layers.Dense(256, activation=relu))\n",
    "        model.add(keras.layers.Dense(self.num_action_space, activation=linear))\n",
    "\n",
    "        # Compiling Model using MSE Loss and Adam Optimizer\n",
    "        model.compile(loss=mean_squared_error, optimizer=Adam(lr=self.lr))\n",
    "\n",
    "        self.model = model\n",
    "        print(model.summary())\n",
    "    \n",
    "    # Chooses an action based on the Epsilon value (Random action Epsilon% of the time)\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.num_action_space)\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    # Trains model based off of Cumulative Training Data\n",
    "    def learn(self):\n",
    "\n",
    "        # Cancels Training if there is insufficient data or if ther model is sufficiently trained\n",
    "        if len(self.training_data) < self.batch_size:\n",
    "            return\n",
    "        if np.mean(self.rewards_list[-10:]) > 180:\n",
    "            return\n",
    "\n",
    "        # Randomly Samples frames out of Training Data based on self.batch_size\n",
    "        sample = random.sample(self.training_data, self.batch_size)\n",
    "        \n",
    "        # Extracts components from each frame and condenses them into arrays\n",
    "        states = np.squeeze(np.squeeze(np.array([i[0] for i in sample])))\n",
    "        actions = np.array([i[1] for i in sample])\n",
    "        rewards = np.array([i[2] for i in sample])\n",
    "        new_states = np.squeeze(np.array([i[3] for i in sample]))\n",
    "        done_list = np.array([i[4] for i in sample])\n",
    "        \n",
    "        # Creates \"targets\" for model.fit()\n",
    "        target_vec = self.model.predict_on_batch(states)\n",
    "        targets = rewards + self.gamma * (np.amax(target_vec, axis=1)) * (1 - done_list)\n",
    "        indexes = np.array([i for i in range(len(states))])\n",
    "        target_vec[[indexes], [actions]] = targets\n",
    "        \n",
    "        self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "\n",
    "    # Handles Generating Training Episodes and Trains Model\n",
    "    def train(self, episodes = 500):\n",
    "        progress = tqdm(total=episodes, position=0, leave=False)\n",
    "        \n",
    "        # Epsiodes Loop\n",
    "        for e in range(episodes):\n",
    "            progress.update(1)\n",
    "\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            MAX_STEPS = 1000\n",
    "            state = np.reshape(state, [1, self.num_observation_space])\n",
    "\n",
    "            # Step Loop\n",
    "            for s in range(MAX_STEPS):\n",
    "                #env.render()\n",
    "\n",
    "                action = self.get_action(state) # Chooses action\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action) # Takes Action and records New State\n",
    "                new_state = np.reshape(new_state, [1, self.num_observation_space])\n",
    "\n",
    "                self.training_data.append((state, action, reward, new_state, done)) # adds information about the fram to training data\n",
    "                \n",
    "                episode_reward += reward # Reward tally\n",
    "\n",
    "                state = new_state #Progressing of game\n",
    "                \n",
    "                self.learn() \n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.rewards_list.append(episode_reward) # Tracks rewards and keeps a high score\n",
    "            if self.high_score < episode_reward:\n",
    "                self.high_score = episode_reward\n",
    "\n",
    "            if self.epsilon > self.epsilon_min: # Handles epsilon decay over the course of the episode\n",
    "                self.epsilon *= self.epsilon_decay #(episodes-e)/episodes \n",
    "            \n",
    "            if np.mean(self.rewards_list[-100:]) > 200: # Stops training if Scores are above 200\n",
    "                print(\"Average Score: 200. Training Completed...\")\n",
    "                break\n",
    "            \n",
    "            \n",
    "    \n",
    "            print(\" || Reward: \", \"%.2f\" % episode_reward, \"\\t|| Average Reward: \", \"%.2f\" % np.mean(self.rewards_list[-100:]), \"\\t epsilon: \", \"%.4f\" % self.epsilon )\n",
    "\n",
    "        print(\"Training Complete...\")\n",
    "        print(\"Highest Training Score:\", self.high_score)\n",
    "\n",
    "    # Saves Model in .h5 format\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 512)               4608      \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               131328    \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 1028      \n=================================================================\nTotal params: 136,964\nTrainable params: 136,964\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "lr = .001\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99\n",
    "\n",
    "model = DQN(env, lr, gamma, epsilon, epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "247.89 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1838/2000 [6:14:35<36:17, 13.44s/it] || Reward:  -293.64 \t|| Average Reward:  -248.91 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1839/2000 [6:14:51<38:15, 14.26s/it] || Reward:  -538.97 \t|| Average Reward:  -251.69 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1840/2000 [6:15:01<34:28, 12.93s/it] || Reward:  -282.17 \t|| Average Reward:  -252.04 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1841/2000 [6:15:15<35:47, 13.51s/it] || Reward:  -263.15 \t|| Average Reward:  -252.11 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1842/2000 [6:15:24<31:53, 12.11s/it] || Reward:  -305.47 \t|| Average Reward:  -252.27 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1843/2000 [6:15:30<26:37, 10.17s/it] || Reward:  -252.65 \t|| Average Reward:  -249.94 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1844/2000 [6:15:46<31:10, 11.99s/it] || Reward:  -133.85 \t|| Average Reward:  -249.04 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1845/2000 [6:15:53<27:15, 10.55s/it] || Reward:  -163.60 \t|| Average Reward:  -249.00 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1846/2000 [6:16:10<31:23, 12.23s/it] || Reward:  -239.84 \t|| Average Reward:  -250.49 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1847/2000 [6:16:18<28:06, 11.03s/it] || Reward:  -71.04 \t|| Average Reward:  -249.70 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1848/2000 [6:16:26<26:09, 10.32s/it] || Reward:  -204.40 \t|| Average Reward:  -249.53 \t epsilon:  0.0100\n",
      " 92%|█████████▏| 1849/2000 [6:16:46<32:51, 13.05s/it] || Reward:  -208.61 \t|| Average Reward:  -250.67 \t epsilon:  0.0100\n",
      " 92%|█████████▎| 1850/2000 [6:16:56<30:47, 12.31s/it] || Reward:  -198.84 \t|| Average Reward:  -250.87 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1851/2000 [6:17:05<27:48, 11.20s/it] || Reward:  -152.23 \t|| Average Reward:  -248.25 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1852/2000 [6:17:12<24:37,  9.98s/it] || Reward:  43.43 \t|| Average Reward:  -245.23 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1853/2000 [6:17:29<29:47, 12.16s/it] || Reward:  33.21 \t|| Average Reward:  -242.08 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1854/2000 [6:17:49<34:55, 14.35s/it] || Reward:  -212.82 \t|| Average Reward:  -242.47 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1855/2000 [6:18:04<35:02, 14.50s/it] || Reward:  -17.59 \t|| Average Reward:  -242.25 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1856/2000 [6:18:17<33:52, 14.12s/it] || Reward:  -258.22 \t|| Average Reward:  -242.64 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1857/2000 [6:18:33<35:09, 14.75s/it] || Reward:  -145.60 \t|| Average Reward:  -243.04 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1858/2000 [6:18:40<28:57, 12.24s/it] || Reward:  -359.51 \t|| Average Reward:  -245.25 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1859/2000 [6:18:56<31:41, 13.48s/it] || Reward:  -252.64 \t|| Average Reward:  -242.79 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1860/2000 [6:19:04<27:29, 11.78s/it] || Reward:  -434.97 \t|| Average Reward:  -242.65 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1861/2000 [6:19:15<26:50, 11.58s/it] || Reward:  -109.12 \t|| Average Reward:  -241.26 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1862/2000 [6:19:23<24:36, 10.70s/it] || Reward:  -226.50 \t|| Average Reward:  -241.42 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1863/2000 [6:19:32<22:52, 10.02s/it] || Reward:  -350.49 \t|| Average Reward:  -240.77 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1864/2000 [6:19:53<29:56, 13.21s/it] || Reward:  271.10 \t|| Average Reward:  -234.06 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1865/2000 [6:20:02<27:15, 12.11s/it] || Reward:  -34.53 \t|| Average Reward:  -230.71 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1866/2000 [6:20:10<24:12, 10.84s/it] || Reward:  -130.79 \t|| Average Reward:  -229.86 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1867/2000 [6:20:24<25:55, 11.69s/it] || Reward:  -121.43 \t|| Average Reward:  -229.30 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1868/2000 [6:20:30<22:01, 10.01s/it] || Reward:  -118.75 \t|| Average Reward:  -227.63 \t epsilon:  0.0100\n",
      " 93%|█████████▎| 1869/2000 [6:20:45<24:57, 11.43s/it] || Reward:  -186.62 \t|| Average Reward:  -228.60 \t epsilon:  0.0100\n",
      " 94%|█████████▎| 1870/2000 [6:21:05<30:27, 14.06s/it] || Reward:  -12.42 \t|| Average Reward:  -225.92 \t epsilon:  0.0100\n",
      " 94%|█████████▎| 1871/2000 [6:21:20<31:19, 14.57s/it] || Reward:  -113.01 \t|| Average Reward:  -225.97 \t epsilon:  0.0100\n",
      " 94%|█████████▎| 1872/2000 [6:21:26<25:30, 11.96s/it] || Reward:  -912.00 \t|| Average Reward:  -231.70 \t epsilon:  0.0100\n",
      " 94%|█████████▎| 1873/2000 [6:21:35<23:26, 11.07s/it] || Reward:  -175.91 \t|| Average Reward:  -232.62 \t epsilon:  0.0100\n",
      " 94%|█████████▎| 1874/2000 [6:21:51<26:03, 12.41s/it] || Reward:  -479.20 \t|| Average Reward:  -235.73 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1875/2000 [6:22:05<26:52, 12.90s/it] || Reward:  -303.96 \t|| Average Reward:  -236.88 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1876/2000 [6:22:25<31:04, 15.04s/it] || Reward:  -549.74 \t|| Average Reward:  -240.67 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1877/2000 [6:22:40<30:35, 14.93s/it] || Reward:  -97.87 \t|| Average Reward:  -239.32 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1878/2000 [6:22:57<31:57, 15.72s/it] || Reward:  -194.99 \t|| Average Reward:  -239.28 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1879/2000 [6:23:10<29:58, 14.86s/it] || Reward:  -163.10 \t|| Average Reward:  -239.53 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1880/2000 [6:23:23<28:26, 14.22s/it] || Reward:  -357.22 \t|| Average Reward:  -239.33 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1881/2000 [6:23:36<27:26, 13.84s/it] || Reward:  -62.27 \t|| Average Reward:  -236.84 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1882/2000 [6:23:45<24:41, 12.55s/it] || Reward:  -186.34 \t|| Average Reward:  -235.07 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1883/2000 [6:23:53<21:30, 11.03s/it] || Reward:  -256.80 \t|| Average Reward:  -232.59 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1884/2000 [6:23:57<17:17,  8.95s/it] || Reward:  -403.74 \t|| Average Reward:  -234.88 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1885/2000 [6:24:05<16:42,  8.72s/it] || Reward:  -509.19 \t|| Average Reward:  -238.84 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1886/2000 [6:24:13<16:10,  8.51s/it] || Reward:  -95.91 \t|| Average Reward:  -234.77 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1887/2000 [6:24:24<17:11,  9.13s/it] || Reward:  -413.82 \t|| Average Reward:  -237.23 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1888/2000 [6:24:40<20:55, 11.21s/it] || Reward:  -274.75 \t|| Average Reward:  -235.61 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1889/2000 [6:24:54<22:12, 12.00s/it] || Reward:  -421.12 \t|| Average Reward:  -235.14 \t epsilon:  0.0100\n",
      " 94%|█████████▍| 1890/2000 [6:25:06<22:06, 12.06s/it] || Reward:  -187.75 \t|| Average Reward:  -236.51 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1891/2000 [6:25:21<23:25, 12.89s/it] || Reward:  -237.00 \t|| Average Reward:  -237.74 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1892/2000 [6:25:31<22:08, 12.30s/it] || Reward:  -438.72 \t|| Average Reward:  -240.73 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1893/2000 [6:25:43<21:24, 12.01s/it] || Reward:  41.01 \t|| Average Reward:  -236.37 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1894/2000 [6:25:55<21:27, 12.15s/it] || Reward:  -492.84 \t|| Average Reward:  -239.07 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1895/2000 [6:26:04<19:12, 10.98s/it] || Reward:  22.21 \t|| Average Reward:  -238.21 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1896/2000 [6:26:11<17:06,  9.87s/it] || Reward:  -216.03 \t|| Average Reward:  -239.06 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1897/2000 [6:26:19<16:18,  9.50s/it] || Reward:  -115.55 \t|| Average Reward:  -234.28 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1898/2000 [6:26:27<15:08,  8.91s/it] || Reward:  -124.65 \t|| Average Reward:  -234.14 \t epsilon:  0.0100\n",
      " 95%|█████████▍| 1899/2000 [6:26:43<18:21, 10.91s/it] || Reward:  -304.76 \t|| Average Reward:  -234.98 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1900/2000 [6:26:53<18:03, 10.83s/it] || Reward:  -464.32 \t|| Average Reward:  -237.45 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1901/2000 [6:27:06<18:59, 11.51s/it] || Reward:  -131.06 \t|| Average Reward:  -233.47 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1902/2000 [6:27:18<18:51, 11.55s/it] || Reward:  -510.54 \t|| Average Reward:  -233.60 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1903/2000 [6:27:34<20:43, 12.82s/it] || Reward:  -185.47 \t|| Average Reward:  -235.52 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1904/2000 [6:27:46<20:09, 12.60s/it] || Reward:  -601.10 \t|| Average Reward:  -238.94 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1905/2000 [6:27:58<19:43, 12.46s/it] || Reward:  -182.39 \t|| Average Reward:  -238.34 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1906/2000 [6:28:07<18:08, 11.58s/it] || Reward:  -239.52 \t|| Average Reward:  -238.51 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1907/2000 [6:28:27<21:49, 14.08s/it] || Reward:  -93.12 \t|| Average Reward:  -237.30 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1908/2000 [6:28:46<23:29, 15.32s/it] || Reward:  -52.98 \t|| Average Reward:  -235.98 \t epsilon:  0.0100\n",
      " 95%|█████████▌| 1909/2000 [6:28:58<21:42, 14.31s/it] || Reward:  -295.03 \t|| Average Reward:  -236.47 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1910/2000 [6:29:14<22:32, 15.03s/it] || Reward:  -90.98 \t|| Average Reward:  -235.89 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1911/2000 [6:29:57<34:47, 23.45s/it] || Reward:  -305.70 \t|| Average Reward:  -236.20 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1912/2000 [6:30:13<31:05, 21.19s/it] || Reward:  -44.08 \t|| Average Reward:  -231.26 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1913/2000 [6:30:29<28:26, 19.61s/it] || Reward:  -240.33 \t|| Average Reward:  -229.14 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1914/2000 [6:30:44<25:49, 18.02s/it] || Reward:  -73.61 \t|| Average Reward:  -228.07 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1915/2000 [6:30:56<23:07, 16.32s/it] || Reward:  -379.44 \t|| Average Reward:  -232.28 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1916/2000 [6:31:09<21:18, 15.22s/it] || Reward:  41.38 \t|| Average Reward:  -229.86 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1917/2000 [6:31:22<20:30, 14.82s/it] || Reward:  18.85 \t|| Average Reward:  -228.78 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1918/2000 [6:31:38<20:25, 14.95s/it] || Reward:  -301.63 \t|| Average Reward:  -226.07 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1919/2000 [6:31:46<17:31, 12.99s/it] || Reward:  -85.17 \t|| Average Reward:  -222.00 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1920/2000 [6:32:00<17:31, 13.14s/it] || Reward:  -183.86 \t|| Average Reward:  -221.19 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1921/2000 [6:32:14<17:57, 13.64s/it] || Reward:  -93.87 \t|| Average Reward:  -218.87 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1922/2000 [6:32:28<17:38, 13.57s/it] || Reward:  -67.77 \t|| Average Reward:  -217.53 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1923/2000 [6:32:39<16:34, 12.92s/it] || Reward:  -136.53 \t|| Average Reward:  -216.42 \t epsilon:  0.0100\n",
      " 96%|█████████▌| 1924/2000 [6:32:51<15:59, 12.62s/it] || Reward:  -582.64 \t|| Average Reward:  -218.29 \t epsilon:  0.0100\n",
      " 96%|█████████▋| 1925/2000 [6:33:04<15:58, 12.77s/it] || Reward:  -667.80 \t|| Average Reward:  -219.39 \t epsilon:  0.0100\n",
      " 96%|█████████▋| 1926/2000 [6:33:21<17:03, 13.83s/it] || Reward:  -600.09 \t|| Average Reward:  -220.19 \t epsilon:  0.0100\n",
      " 96%|█████████▋| 1927/2000 [6:33:28<14:27, 11.89s/it] || Reward:  -332.42 \t|| Average Reward:  -220.58 \t epsilon:  0.0100\n",
      " 96%|█████████▋| 1928/2000 [6:33:39<14:09, 11.80s/it] || Reward:  -134.95 \t|| Average Reward:  -221.35 \t epsilon:  0.0100\n",
      " 96%|█████████▋| 1929/2000 [6:33:52<14:06, 11.92s/it] || Reward:  -236.55 \t|| Average Reward:  -223.30 \t epsilon:  0.0100\n",
      " 96%|█████████▋| 1930/2000 [6:34:09<15:39, 13.42s/it] || Reward:  -239.78 \t|| Average Reward:  -224.11 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1931/2000 [6:34:21<15:04, 13.11s/it] || Reward:  -118.49 \t|| Average Reward:  -223.55 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1932/2000 [6:34:29<13:07, 11.59s/it] || Reward:  -201.59 \t|| Average Reward:  -224.48 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1933/2000 [6:34:52<16:35, 14.86s/it] || Reward:  -218.45 \t|| Average Reward:  -225.58 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1934/2000 [6:35:01<14:34, 13.25s/it] || Reward:  -482.95 \t|| Average Reward:  -228.39 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1935/2000 [6:35:09<12:31, 11.56s/it] || Reward:  -483.02 \t|| Average Reward:  -230.72 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1936/2000 [6:35:17<11:18, 10.61s/it] || Reward:  -150.60 \t|| Average Reward:  -232.14 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1937/2000 [6:35:28<11:17, 10.75s/it] || Reward:  -130.90 \t|| Average Reward:  -232.36 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1938/2000 [6:35:39<11:05, 10.73s/it] || Reward:  -62.78 \t|| Average Reward:  -230.05 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1939/2000 [6:35:51<11:14, 11.06s/it] || Reward:  -175.57 \t|| Average Reward:  -226.42 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1940/2000 [6:36:00<10:33, 10.56s/it] || Reward:  -448.75 \t|| Average Reward:  -228.09 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1941/2000 [6:36:10<10:12, 10.39s/it] || Reward:  -33.70 \t|| Average Reward:  -225.79 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1942/2000 [6:36:25<11:21, 11.74s/it] || Reward:  -301.94 \t|| Average Reward:  -225.76 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1943/2000 [6:36:43<12:53, 13.57s/it] || Reward:  -199.29 \t|| Average Reward:  -225.22 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1944/2000 [6:36:51<11:06, 11.91s/it] || Reward:  -226.01 \t|| Average Reward:  -226.14 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1945/2000 [6:37:02<10:45, 11.74s/it] || Reward:  -82.13 \t|| Average Reward:  -225.33 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1946/2000 [6:37:12<09:59, 11.10s/it] || Reward:  -276.94 \t|| Average Reward:  -225.70 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1947/2000 [6:37:21<09:12, 10.42s/it] || Reward:  -274.99 \t|| Average Reward:  -227.74 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1948/2000 [6:37:32<09:16, 10.70s/it] || Reward:  -262.78 \t|| Average Reward:  -228.32 \t epsilon:  0.0100\n",
      " 97%|█████████▋| 1949/2000 [6:37:44<09:22, 11.04s/it] || Reward:  -256.33 \t|| Average Reward:  -228.80 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1950/2000 [6:38:00<10:35, 12.71s/it] || Reward:  -422.57 \t|| Average Reward:  -231.04 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1951/2000 [6:38:08<09:13, 11.30s/it] || Reward:  -101.42 \t|| Average Reward:  -230.53 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1952/2000 [6:38:22<09:33, 11.96s/it] || Reward:  -21.76 \t|| Average Reward:  -231.18 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1953/2000 [6:38:29<08:21, 10.66s/it] || Reward:  -394.84 \t|| Average Reward:  -235.46 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1954/2000 [6:38:37<07:31,  9.82s/it] || Reward:  -373.90 \t|| Average Reward:  -237.07 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1955/2000 [6:39:00<10:16, 13.70s/it] || Reward:  -350.62 \t|| Average Reward:  -240.40 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1956/2000 [6:39:08<08:44, 11.93s/it] || Reward:  -58.01 \t|| Average Reward:  -238.40 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1957/2000 [6:39:21<08:46, 12.26s/it] || Reward:  -324.92 \t|| Average Reward:  -240.19 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1958/2000 [6:39:30<08:00, 11.43s/it] || Reward:  -116.02 \t|| Average Reward:  -237.76 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1959/2000 [6:39:50<09:31, 13.94s/it] || Reward:  -126.83 \t|| Average Reward:  -236.50 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1960/2000 [6:40:01<08:42, 13.05s/it] || Reward:  -306.31 \t|| Average Reward:  -235.21 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1961/2000 [6:40:24<10:27, 16.09s/it] || Reward:  -206.31 \t|| Average Reward:  -236.19 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1962/2000 [6:40:32<08:34, 13.55s/it] || Reward:  -185.45 \t|| Average Reward:  -235.78 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1963/2000 [6:40:49<08:56, 14.49s/it] || Reward:  -400.84 \t|| Average Reward:  -236.28 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1964/2000 [6:41:02<08:27, 14.09s/it] || Reward:  -299.72 \t|| Average Reward:  -241.99 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1965/2000 [6:41:14<07:53, 13.52s/it] || Reward:  -369.68 \t|| Average Reward:  -245.34 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1966/2000 [6:41:32<08:20, 14.73s/it] || Reward:  -0.25 \t|| Average Reward:  -244.03 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1967/2000 [6:41:53<09:11, 16.72s/it] || Reward:  -26.94 \t|| Average Reward:  -243.09 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1968/2000 [6:42:02<07:41, 14.43s/it] || Reward:  -57.59 \t|| Average Reward:  -242.48 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1969/2000 [6:42:06<05:48, 11.24s/it] || Reward:  -426.09 \t|| Average Reward:  -244.87 \t epsilon:  0.0100\n",
      " 98%|█████████▊| 1970/2000 [6:42:19<05:53, 11.79s/it] || Reward:  -60.93 \t|| Average Reward:  -245.36 \t epsilon:  0.0100\n",
      " 99%|█████████▊| 1971/2000 [6:42:35<06:17, 13.01s/it] || Reward:  -159.03 \t|| Average Reward:  -245.82 \t epsilon:  0.0100\n",
      " 99%|█████████▊| 1972/2000 [6:42:52<06:40, 14.30s/it] || Reward:  -85.22 \t|| Average Reward:  -237.55 \t epsilon:  0.0100\n",
      " 99%|█████████▊| 1973/2000 [6:43:10<06:54, 15.34s/it] || Reward:  -47.49 \t|| Average Reward:  -236.27 \t epsilon:  0.0100\n",
      " 99%|█████████▊| 1974/2000 [6:43:19<05:53, 13.59s/it] || Reward:  -121.22 \t|| Average Reward:  -232.69 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1975/2000 [6:43:53<08:10, 19.61s/it] || Reward:  -249.05 \t|| Average Reward:  -232.14 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1976/2000 [6:44:17<08:23, 20.97s/it] || Reward:  -126.22 \t|| Average Reward:  -227.90 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1977/2000 [6:44:31<07:14, 18.90s/it] || Reward:  -181.31 \t|| Average Reward:  -228.74 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1978/2000 [6:44:44<06:18, 17.19s/it] || Reward:  -303.35 \t|| Average Reward:  -229.82 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1979/2000 [6:45:05<06:22, 18.19s/it] || Reward:  -202.86 \t|| Average Reward:  -230.22 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1980/2000 [6:45:24<06:06, 18.33s/it] || Reward:  -363.42 \t|| Average Reward:  -230.28 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1981/2000 [6:46:02<07:44, 24.47s/it] || Reward:  -355.91 \t|| Average Reward:  -233.22 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1982/2000 [6:46:11<05:56, 19.83s/it] || Reward:  -120.62 \t|| Average Reward:  -232.56 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1983/2000 [6:46:25<05:04, 17.92s/it] || Reward:  -203.63 \t|| Average Reward:  -232.03 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1984/2000 [6:47:18<07:35, 28.48s/it] || Reward:  -148.92 \t|| Average Reward:  -229.48 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1985/2000 [6:47:37<06:23, 25.55s/it] || Reward:  -77.75 \t|| Average Reward:  -225.16 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1986/2000 [6:47:48<04:57, 21.28s/it] || Reward:  -150.72 \t|| Average Reward:  -225.71 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1987/2000 [6:48:08<04:32, 20.97s/it] || Reward:  -260.03 \t|| Average Reward:  -224.17 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1988/2000 [6:48:15<03:22, 16.84s/it] || Reward:  -175.74 \t|| Average Reward:  -223.18 \t epsilon:  0.0100\n",
      " 99%|█████████▉| 1989/2000 [6:48:35<03:13, 17.56s/it] || Reward:  -186.00 \t|| Average Reward:  -220.83 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1990/2000 [6:48:41<02:20, 14.05s/it] || Reward:  -739.63 \t|| Average Reward:  -226.35 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1991/2000 [6:48:50<01:54, 12.76s/it] || Reward:  -47.09 \t|| Average Reward:  -224.45 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1992/2000 [6:48:59<01:31, 11.41s/it] || Reward:  -50.54 \t|| Average Reward:  -220.57 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1993/2000 [6:49:07<01:13, 10.54s/it] || Reward:  -806.24 \t|| Average Reward:  -229.04 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1994/2000 [6:49:16<01:00, 10.16s/it] || Reward:  -110.10 \t|| Average Reward:  -225.22 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1995/2000 [6:49:33<01:00, 12.04s/it] || Reward:  -358.35 \t|| Average Reward:  -229.02 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1996/2000 [6:49:45<00:48, 12.07s/it] || Reward:  -171.76 \t|| Average Reward:  -228.58 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1997/2000 [6:49:53<00:32, 10.87s/it] || Reward:  -67.58 \t|| Average Reward:  -228.10 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1998/2000 [6:50:15<00:28, 14.19s/it] || Reward:  -199.71 \t|| Average Reward:  -228.85 \t epsilon:  0.0100\n",
      "100%|█████████▉| 1999/2000 [6:50:34<00:15, 15.65s/it] || Reward:  -255.68 \t|| Average Reward:  -228.36 \t epsilon:  0.0100\n",
      "100%|██████████| 2000/2000 [6:50:55<00:00, 17.23s/it] || Reward:  -289.72 \t|| Average Reward:  -226.61 \t epsilon:  0.0100\n",
      " || Reward:  -210.89 \t|| Average Reward:  -227.41 \t epsilon:  0.0100\n",
      "Training Complete...\n",
      "Highest Training Score: 280.1251846858248\n"
     ]
    }
   ],
   "source": [
    "model.train(episodes = 2000)\n",
    "model.save(\"LLtrainedmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Testing of the trained model...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-85fa2b1c9819>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mrewards_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Keeps track of Scores and High Score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_score\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "trained_model = load_model(\"LLmodel6.h5\") #chooses model to run\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "rewards_list = []\n",
    "high_score = -8000\n",
    "MAX_EPSIODES = 10\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "print(\"Starting Testing of the trained model...\")\n",
    "for e in range(MAX_EPSIODES):\n",
    "    state = env.reset()\n",
    "    num_observation_space = env.observation_space.shape[0]\n",
    "    state = np.reshape(state, [1, num_observation_space])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for s in range(MAX_STEPS):\n",
    "        env.render() # Renders Environment with Box2D\n",
    "\n",
    "        action = np.argmax(trained_model.predict(state)[0]) # Executes \"best\" action for given state using trained_model's prediction\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_state = np.reshape(new_state, [1, num_observation_space])\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        episode_reward += reward # Reward Tally\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards_list.append(episode_reward) # Keeps track of Scores and High Score\n",
    "    if high_score < episode_reward:\n",
    "        high_score = episode_reward\n",
    "\n",
    "    print(e, \"\\t: Episode || Reward: \", \"%.2f\" % episode_reward, \"\\t|| Average Reward: \", \"%.2f\" % np.mean(rewards_list[-100:]))\n",
    "    \n",
    "print(\"Testing Complete...\")\n",
    "print(\"Highest Testing Score:\", high_score)\n",
    "\n",
    "env.close() # Shuts Down Environment"
   ]
  },
  {
   "source": [
    "# Saved Model Logs\n",
    "\n",
    "1. Threshold Set to +300. No episodes accepted into training set. Model was making randomized actions\n",
    "2. Threshold set to -200. 500 Episodes. Best model so far. It is able to control its vertical velocity well, but is still shaky on        roll and targetting the pad (High Score: 270.715)\n",
    "3. Threshold set to -200. 1000 Epsiodes.\n",
    "4. Reformatted all the code. Training is now done per step rather than per episode. All steps are accepted, but training_data will only hold the 500,000 most recent steps. Great improvements in consistency.\n",
    "5. Changed LR to .001, 500 episodes\n",
    "6. Running 2000 Episodes, same hyperparameters. (High Score: 280.1251846858248)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}